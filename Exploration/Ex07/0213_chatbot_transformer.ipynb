{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0213-chatbot.ipynb  data\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "datapath = os.getenv('HOME')+'/aiffel/transformer_chatbot/data/ChatbotData.csv'\n",
    "data = pd.read_csv(datapath)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11823"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q        0\n",
       "A        0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "  # 단어와 구두점(punctuation) 사이의 공백 추가\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  # 연속된 공백을 하나의 공백으로 전환\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12시 땡!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Q'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12시 땡 !'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(data['Q'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_q'] = data['Q'].apply(lambda x : preprocess_sentence(x))\n",
    "data['clean_a'] = data['A'].apply(lambda x : preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "      <th>clean_q</th>\n",
       "      <th>clean_a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "      <td>12시 땡 !</td>\n",
       "      <td>하루가 또 가네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠 .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label          clean_q       clean_a\n",
       "0           12시 땡!   하루가 또 가네요.      0          12시 땡 !   하루가 또 가네요 .\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0      1지망 학교 떨어졌어    위로해 드립니다 .\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0     3박4일 놀러가고 싶다  여행은 언제나 좋죠 .\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠 .\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0          PPL 심하네   눈살이 찌푸려지죠 ."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                         12시 땡 !\n",
       " 1                     1지망 학교 떨어졌어\n",
       " 2                    3박4일 놀러가고 싶다\n",
       " 3                 3박4일 정도 놀러가고 싶다\n",
       " 4                         PPL 심하네\n",
       "                    ...           \n",
       " 11818             훔쳐보는 것도 눈치 보임 .\n",
       " 11819             훔쳐보는 것도 눈치 보임 .\n",
       " 11820                흑기사 해주는 짝남 .\n",
       " 11821    힘든 연애 좋은 연애라는게 무슨 차이일까 ?\n",
       " 11822                  힘들어서 결혼할까봐\n",
       " Name: clean_q, Length: 11823, dtype: object,\n",
       " 0                      하루가 또 가네요 .\n",
       " 1                       위로해 드립니다 .\n",
       " 2                     여행은 언제나 좋죠 .\n",
       " 3                     여행은 언제나 좋죠 .\n",
       " 4                      눈살이 찌푸려지죠 .\n",
       "                    ...            \n",
       " 11818          티가 나니까 눈치가 보이는 거죠 !\n",
       " 11819               훔쳐보는 거 티나나봐요 .\n",
       " 11820                      설렜겠어요 .\n",
       " 11821    잘 헤어질 수 있는 사이 여부인 거 같아요 .\n",
       " 11822          도피성 결혼은 하지 않길 바라요 .\n",
       " Name: clean_a, Length: 11823, dtype: object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = data['clean_q']\n",
    "answers = data['clean_a']\n",
    "questions, answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. SubwordTextEncoder 사용하기 (단어장 만들기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    questions + answers, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [8364]\n",
      "END_TOKEN의 번호 : [8365]\n",
      "단어장의 크기 : 8366\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])\n",
    "print('단어장의 크기 :',VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정수 인코딩 & 패딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 질문 샘플: [5829, 605, 2500, 4174]\n",
      "정수 인코딩 후의 21번째 답변 샘플: [2685, 7669, 8, 6378, 95, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "print('정수 인코딩 후의 21번째 질문 샘플: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('정수 인코딩 후의 21번째 답변 샘플: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 문장 [5829, 605, 2500, 4174]\n",
      "기존 문장: 가스비 장난 아님\n",
      "정수 인코딩 후의 문장 [2685, 7669, 8, 6378, 95, 1]\n",
      "기존 문장: 다음 달에는 더 절약해봐요 .\n"
     ]
    }
   ],
   "source": [
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(questions[21])\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))\n",
    "\n",
    "# encode() : 텍스트 시퀀스 --> 정수 시퀀스\n",
    "tokenized_string = tokenizer.encode(answers[21])\n",
    "print ('정수 인코딩 후의 문장 {}'.format(tokenized_string))\n",
    "\n",
    "# decode() : 정수 시퀀스 --> 텍스트 시퀀스\n",
    "original_string = tokenizer.decode(tokenized_string)\n",
    "print ('기존 문장: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12시 땡 !'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 최대 길이를 40으로 정의\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    tokenized_inputs.append(sentence1)\n",
    "    tokenized_outputs.append(sentence2)\n",
    "\n",
    "  # 패딩\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "  return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8366\n",
      "필터링 후의 질문 샘플 개수: 11823\n",
      "필터링 후의 답변 샘플 개수: 11823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11823, 40), (11823, 40))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(questions)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(answers)))\n",
    "questions.shape, answers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teacher forcing 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
    "# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1] # 마지막 패딩 토큰 제거\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]  # 시작 토큰이 제거\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. 모델 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PositionalEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sin과 cosine이 교차되도록 재배열\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0])\n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    # angle_rads = np.zeros(angle_rads.shape)\n",
    "    # angle_rads[:, 0::2] = sines\n",
    "    # angle_rads[:, 1::2] = cosines\n",
    "    # pos_encoding = tf.constant(angle_rads)\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax적용\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, V에 각각 Dense를 적용합니다\n",
    "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
    "    # q : (batch_size, query의 문장 길이, d_model)\n",
    "    # k : (batch_size, key의 문장 길이, d_model)\n",
    "    # v : (batch_size, value의 문장 길이, d_model)\n",
    "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
    "    query = self.query_dense(query)\n",
    "    key = self.key_dense(key)\n",
    "    value = self.value_dense(value)\n",
    "\n",
    "    # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "    # 2. 헤드 나누기\n",
    "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
    "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
    "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
    "    query = self.split_heads(query, batch_size)\n",
    "    key = self.split_heads(key, batch_size)\n",
    "    value = self.split_heads(value, batch_size)\n",
    "\n",
    "    # 스케일드 닷 프로덕트 어텐션 함수\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create_padding_mask & create_look_ahead_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  # 룩어헤드 마스크\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  # 패딩 마스크\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  # 마스크 두 개 합치기\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 임베딩 레이어\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빠른 버전 vs 논문버전(num_layers는 6, d-Model은 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 256)    3195904     inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3723264     dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8366)   2150062     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,069,230\n",
      "Trainable params: 9,069,230\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수\n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Custom Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 10s 52ms/step - loss: 0.4271 - accuracy: 0.1042\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.3459 - accuracy: 0.1155\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 10s 52ms/step - loss: 0.2709 - accuracy: 0.1263\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 10s 52ms/step - loss: 0.2054 - accuracy: 0.1366\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 10s 52ms/step - loss: 0.1513 - accuracy: 0.1459\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 10s 52ms/step - loss: 0.1092 - accuracy: 0.1533\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0795 - accuracy: 0.1589\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0612 - accuracy: 0.1622\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0508 - accuracy: 0.1640\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0454 - accuracy: 0.1647\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0431 - accuracy: 0.1648\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0398 - accuracy: 0.1657\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0367 - accuracy: 0.1662\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0312 - accuracy: 0.1675\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0286 - accuracy: 0.1682\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0251 - accuracy: 0.1690\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0220 - accuracy: 0.1699\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0203 - accuracy: 0.1701\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0184 - accuracy: 0.1709\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0168 - accuracy: 0.1712\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0151 - accuracy: 0.1715\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0144 - accuracy: 0.1717\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0135 - accuracy: 0.1721\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0124 - accuracy: 0.1722\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0115 - accuracy: 0.1726\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0108 - accuracy: 0.1728\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0103 - accuracy: 0.1728\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0101 - accuracy: 0.1728\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0094 - accuracy: 0.1731\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0090 - accuracy: 0.1731\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0085 - accuracy: 0.1732\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0077 - accuracy: 0.1735\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0075 - accuracy: 0.1735\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0073 - accuracy: 0.1736\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0067 - accuracy: 0.1737\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0064 - accuracy: 0.1737\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0067 - accuracy: 0.1736\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0061 - accuracy: 0.1738\n",
      "Epoch 39/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0059 - accuracy: 0.1739\n",
      "Epoch 40/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0055 - accuracy: 0.1739\n",
      "Epoch 41/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0058 - accuracy: 0.1739\n",
      "Epoch 42/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0049 - accuracy: 0.1742\n",
      "Epoch 43/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0053 - accuracy: 0.1741\n",
      "Epoch 44/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0049 - accuracy: 0.1741\n",
      "Epoch 45/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0047 - accuracy: 0.1741\n",
      "Epoch 46/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0046 - accuracy: 0.1742\n",
      "Epoch 47/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0043 - accuracy: 0.1742\n",
      "Epoch 48/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0049 - accuracy: 0.1742\n",
      "Epoch 49/50\n",
      "185/185 [==============================] - 10s 53ms/step - loss: 0.0044 - accuracy: 0.1742\n",
      "Epoch 50/50\n",
      "185/185 [==============================] - 10s 54ms/step - loss: 0.0041 - accuracy: 0.1743\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence, custom_model):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "    predictions = custom_model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence, custom_model):\n",
    "  # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴\n",
    "  prediction = decoder_inference(sentence, custom_model)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('입력 : {}'.format(sentence))\n",
    "  print('출력 : {}\\n'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 아 심심해\n",
      "출력 : 노래 불러 드릴까요 ? 북치기박치기 헥헥\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'노래 불러 드릴까요 ? 북치기박치기 헥헥'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('아 심심해', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 어 너무 싫어\n",
      "출력 : 잠시 눈을 감고 휴식을 취해보세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'잠시 눈을 감고 휴식을 취해보세요 .'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('어 너무 싫어', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요~~~\n",
      "출력 : 아직 많이 지쳤나봐요 .\n",
      "\n",
      "입력 : 아 퇴근 시간 언제야?\n",
      "출력 : 그래도 먹으려고 노력해보세요 .\n",
      "\n",
      "입력 : 왤케 시간이 안 가냐 ㅠ\n",
      "출력 : 그 사람도 그럴 거예요 .\n",
      "\n",
      "입력 : 집에 가고 싶다\n",
      "출력 : 맛있게 드세요 .\n",
      "\n",
      "입력 : 몇 시에 만날래?\n",
      "출력 : 시간 있냐고 물어보세요 .\n",
      "\n",
      "입력 : 근황 토크 좀 하자\n",
      "출력 : 잠시 눈을 감고 휴식을 취해보세요 .\n",
      "\n",
      "입력 : 거기 개쩌는데잖아\n",
      "출력 : 새로운 데이트 신청 해보세요 .\n",
      "\n",
      "입력 : 나는 헬스를 다니니까\n",
      "출력 : 그 사람도 그럴 수 있어요 .\n",
      "\n",
      "입력 : 니 이제 인났나?\n",
      "출력 : 그 사람을 잊는다는 건 어떨까요 .\n",
      "\n",
      "입력 : 뭐고?\n",
      "출력 : 저는 위로해드리는 로봇이에요 .\n",
      "\n",
      "입력 : 잤나?\n",
      "출력 : 사랑은 소유하는 게 아니에요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    '안녕하세요~~~',\n",
    "    '아 퇴근 시간 언제야?',\n",
    "    '왤케 시간이 안 가냐 ㅠ',\n",
    "    '집에 가고 싶다',\n",
    "    '몇 시에 만날래?',\n",
    "    '근황 토크 좀 하자',\n",
    "    '거기 개쩌는데잖아',\n",
    "    '나는 헬스를 다니니까',\n",
    "    '니 이제 인났나?',\n",
    "    '뭐고?',\n",
    "    '잤나?'\n",
    "]\n",
    "\n",
    "for test in test_questions:\n",
    "    sentence_generation(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**50 Epochs** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요~~~\n",
      "출력 : 안녕하세요 .\n",
      "\n",
      "입력 : 아 퇴근 시간 언제야?\n",
      "출력 : 째깍째깍 .\n",
      "\n",
      "입력 : 왤케 시간이 안 가냐 ㅠ\n",
      "출력 : 다른 남자는 만나봐야 알죠 .\n",
      "\n",
      "입력 : 집에 가고 싶다\n",
      "출력 : 집이 최고죠 .\n",
      "\n",
      "입력 : 몇 시에 만날래?\n",
      "출력 : 사랑은 안 변하고 사람이 변해요 .\n",
      "\n",
      "입력 : 근황 토크 좀 하자\n",
      "출력 : 꾸준히 연락을 하는게 필요합니다 .\n",
      "\n",
      "입력 : 거기 개쩌는데잖아\n",
      "출력 : 세다고 너무 많이 드시면 안돼요 .\n",
      "\n",
      "입력 : 나는 헬스를 다니니까\n",
      "출력 : 시간이 벌써 흘렀네요 .\n",
      "\n",
      "입력 : 니 이제 인났나?\n",
      "출력 : 최신 영화가 좋을 것 같아요 .\n",
      "\n",
      "입력 : 뭐고?\n",
      "출력 : 저는 위로봇입니다 .\n",
      "\n",
      "입력 : 잤나?\n",
      "출력 : 잠이 정말 중요해요 . 눈을 좀 붙이세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in test_questions:\n",
    "    sentence_generation(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 요즘 너무 지쳤어. 어떻게 해야 할까?\n",
      "출력 : 세상에는 흥미로운 일이 많은걸요 .\n",
      "\n",
      "입력 : 내가 잘하고 있는지 모르겠어.\n",
      "출력 : 더 열심히 노력하겠습니다 .\n",
      "\n",
      "입력 : 아무리 노력해도 결과가 안 좋아. 나한테 문제가 있는 걸까?\n",
      "출력 : 다른 사람이 답답할 거예요 .\n",
      "\n",
      "입력 : 사람들이 날 싫어하는 것 같아.\n",
      "출력 : 좀 더 호감을 표현해보세요 .\n",
      "\n",
      "입력 : 계속 실패만 해. 포기해야 할까?\n",
      "출력 : 더 좋은 사람 만날 거예요 .\n",
      "\n",
      "입력 : 누군가에게 기대고 싶은데, 그럴 사람이 없어.\n",
      "출력 : 쉬운 일이 아니긴 하죠 .\n",
      "\n",
      "입력 : 내 이야기를 들어주는 사람이 없어서 외로워.\n",
      "출력 : 조금 더 사랑해보세요 .\n",
      "\n",
      "입력 : 열심히 하는데도 인정받지 못하는 기분이야.\n",
      "출력 : 다음에는 잘 될 거예요 .\n",
      "\n",
      "입력 : 나는 왜 이렇게 부족할까?\n",
      "출력 : 더 노력해보세요 .\n",
      "\n",
      "입력 : 다들 행복해 보이는데, 나만 힘든 것 같아.\n",
      "출력 : 자신만 행복하면 돼요 .\n",
      "\n",
      "입력 : 내 미래가 불안해서 걱정돼.\n",
      "출력 : 또 다른 미래가 펼쳐질 거예요 .\n",
      "\n",
      "입력 : 오늘 너무 힘든 일이 있었어. 위로가 필요해.\n",
      "출력 : 너무 힘들게 하는 사실이네요 .\n",
      "\n",
      "입력 : 무기력하고 아무것도 하기 싫어.\n",
      "출력 : 자신의 감정을 주변 사람들에게 터놓고 이야기해보세요 .\n",
      "\n",
      "입력 : 나는 왜 항상 남들보다 뒤처지는 느낌이지?\n",
      "출력 : 정신 차리세요 .\n",
      "\n",
      "입력 : 요즘 너무 스트레스 받아서 견디기 힘들어.\n",
      "출력 : 운동을 해보세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wellness_questions = [\n",
    "    \"나 요즘 너무 지쳤어. 어떻게 해야 할까?\",\n",
    "    \"내가 잘하고 있는지 모르겠어.\",\n",
    "    \"아무리 노력해도 결과가 안 좋아. 나한테 문제가 있는 걸까?\",\n",
    "    \"사람들이 날 싫어하는 것 같아.\",\n",
    "    \"계속 실패만 해. 포기해야 할까?\",\n",
    "    \"누군가에게 기대고 싶은데, 그럴 사람이 없어.\",\n",
    "    \"내 이야기를 들어주는 사람이 없어서 외로워.\",\n",
    "    \"열심히 하는데도 인정받지 못하는 기분이야.\",\n",
    "    \"나는 왜 이렇게 부족할까?\",\n",
    "    \"다들 행복해 보이는데, 나만 힘든 것 같아.\",\n",
    "    \"내 미래가 불안해서 걱정돼.\",\n",
    "    \"오늘 너무 힘든 일이 있었어. 위로가 필요해.\",\n",
    "    \"무기력하고 아무것도 하기 싫어.\",\n",
    "    \"나는 왜 항상 남들보다 뒤처지는 느낌이지?\",\n",
    "    \"요즘 너무 스트레스 받아서 견디기 힘들어.\"\n",
    "]\n",
    "\n",
    "for test in wellness_questions:\n",
    "    sentence_generation(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**페이퍼 모델 테스트**  (num_layers는 6, d-Model은 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Functional)            (None, None, 512)    13751296    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 512)    20061184    dec_inputs[0][0]                 \n",
      "                                                                 encoder[0][0]                    \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8366)   4291758     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,104,238\n",
      "Trainable params: 38,104,238\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 6 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 512 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수\n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "paper_model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "paper_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "paper_model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "185/185 [==============================] - 54s 207ms/step - loss: 1.3523 - accuracy: 0.0234\n",
      "Epoch 2/10\n",
      "185/185 [==============================] - 39s 211ms/step - loss: 1.0748 - accuracy: 0.0495\n",
      "Epoch 3/10\n",
      "185/185 [==============================] - 40s 214ms/step - loss: 0.9833 - accuracy: 0.0508\n",
      "Epoch 4/10\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.9407 - accuracy: 0.0529\n",
      "Epoch 5/10\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.9059 - accuracy: 0.0552\n",
      "Epoch 6/10\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.8704 - accuracy: 0.0569\n",
      "Epoch 7/10\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.8311 - accuracy: 0.0591\n",
      "Epoch 8/10\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.7840 - accuracy: 0.0619\n",
      "Epoch 9/10\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.7334 - accuracy: 0.0655\n",
      "Epoch 10/10\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.6777 - accuracy: 0.0703\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "history = paper_model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['accuracy'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요~~~\n",
      "출력 : 좋은 사람 만날 거예요 .\n",
      "\n",
      "입력 : 아 퇴근 시간 언제야?\n",
      "출력 : 저는 잘 될 거예요 .\n",
      "\n",
      "입력 : 왤케 시간이 안 가냐 ㅠ\n",
      "출력 : 이제 좀 더 괜찮아졌길 바랄게요 .\n",
      "\n",
      "입력 : 집에 가고 싶다\n",
      "출력 : 맛있게 드세요 .\n",
      "\n",
      "입력 : 몇 시에 만날래?\n",
      "출력 : 직접 물어보세요 .\n",
      "\n",
      "입력 : 근황 토크 좀 하자\n",
      "출력 : 좋은 사람 만날 수 있을 거예요 .\n",
      "\n",
      "입력 : 거기 개쩌는데잖아\n",
      "출력 : 잘 되길 바랄게요 .\n",
      "\n",
      "입력 : 나는 헬스를 다니니까\n",
      "출력 : 좋은 사람 만날 수 있을 거예요 .\n",
      "\n",
      "입력 : 니 이제 인났나?\n",
      "출력 : 직접 물어보세요 .\n",
      "\n",
      "입력 : 뭐고?\n",
      "출력 : 같이 가보세요 .\n",
      "\n",
      "입력 : 잤나?\n",
      "출력 : 저도 좋아해요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in test_questions:\n",
    "    sentence_generation(test, paper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 요즘 너무 지쳤어. 어떻게 해야 할까?\n",
      "출력 : 혼자 사는 것도 좋을 것 같아요 .\n",
      "\n",
      "입력 : 내가 잘하고 있는지 모르겠어.\n",
      "출력 : 잘 될 거예요 .\n",
      "\n",
      "입력 : 아무리 노력해도 결과가 안 좋아. 나한테 문제가 있는 걸까?\n",
      "출력 : 직접 물어보세요 .\n",
      "\n",
      "입력 : 사람들이 날 싫어하는 것 같아.\n",
      "출력 : 이제 좀 괜찮아졌길 바랄게요 .\n",
      "\n",
      "입력 : 계속 실패만 해. 포기해야 할까?\n",
      "출력 : 직접 물어보세요 .\n",
      "\n",
      "입력 : 누군가에게 기대고 싶은데, 그럴 사람이 없어.\n",
      "출력 : 마음이 복잡하겠어요 .\n",
      "\n",
      "입력 : 내 이야기를 들어주는 사람이 없어서 외로워.\n",
      "출력 : 직접 물어보세요 .\n",
      "\n",
      "입력 : 열심히 하는데도 인정받지 못하는 기분이야.\n",
      "출력 : 잘 될 거예요 .\n",
      "\n",
      "입력 : 나는 왜 이렇게 부족할까?\n",
      "출력 : 혼자 사는 것도 좋을 것 같아요 .\n",
      "\n",
      "입력 : 다들 행복해 보이는데, 나만 힘든 것 같아.\n",
      "출력 : 그 사람도 가장 가장 가장 좋은 사람 만날 수 있을 거예요 .\n",
      "\n",
      "입력 : 내 미래가 불안해서 걱정돼.\n",
      "출력 : 그 사람도 그럴 거예요 .\n",
      "\n",
      "입력 : 오늘 너무 힘든 일이 있었어. 위로가 필요해.\n",
      "출력 : 잘 될 거예요 .\n",
      "\n",
      "입력 : 무기력하고 아무것도 하기 싫어.\n",
      "출력 : 직접 물어보세요 .\n",
      "\n",
      "입력 : 나는 왜 항상 남들보다 뒤처지는 느낌이지?\n",
      "출력 : 직접 물어보는 건 어떨까요 .\n",
      "\n",
      "입력 : 요즘 너무 스트레스 받아서 견디기 힘들어.\n",
      "출력 : 좋은 생각이에요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in wellness_questions:\n",
    "    sentence_generation(test, paper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "185/185 [==============================] - 38s 206ms/step - loss: 0.6184 - accuracy: 0.0765\n",
      "Epoch 2/50\n",
      "185/185 [==============================] - 39s 210ms/step - loss: 0.5594 - accuracy: 0.0832\n",
      "Epoch 3/50\n",
      "185/185 [==============================] - 39s 213ms/step - loss: 0.5023 - accuracy: 0.0902\n",
      "Epoch 4/50\n",
      "185/185 [==============================] - 40s 216ms/step - loss: 0.4460 - accuracy: 0.0973\n",
      "Epoch 5/50\n",
      "185/185 [==============================] - 40s 219ms/step - loss: 0.3976 - accuracy: 0.1037\n",
      "Epoch 6/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.3536 - accuracy: 0.1098\n",
      "Epoch 7/50\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.3199 - accuracy: 0.1139\n",
      "Epoch 8/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.2914 - accuracy: 0.1174\n",
      "Epoch 9/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.2705 - accuracy: 0.1201\n",
      "Epoch 10/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.2516 - accuracy: 0.1224\n",
      "Epoch 11/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.2365 - accuracy: 0.1249\n",
      "Epoch 12/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.2253 - accuracy: 0.1268\n",
      "Epoch 13/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.2075 - accuracy: 0.1292\n",
      "Epoch 14/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1907 - accuracy: 0.1324\n",
      "Epoch 15/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1732 - accuracy: 0.1353\n",
      "Epoch 16/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1587 - accuracy: 0.1378\n",
      "Epoch 17/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1493 - accuracy: 0.1397\n",
      "Epoch 18/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1349 - accuracy: 0.1424\n",
      "Epoch 19/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1270 - accuracy: 0.1438\n",
      "Epoch 20/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1168 - accuracy: 0.1459\n",
      "Epoch 21/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1081 - accuracy: 0.1479\n",
      "Epoch 22/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.1003 - accuracy: 0.1492\n",
      "Epoch 23/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0938 - accuracy: 0.1508\n",
      "Epoch 24/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0860 - accuracy: 0.1526\n",
      "Epoch 25/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0819 - accuracy: 0.1535\n",
      "Epoch 26/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0759 - accuracy: 0.1550\n",
      "Epoch 27/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0710 - accuracy: 0.1561\n",
      "Epoch 28/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0666 - accuracy: 0.1571\n",
      "Epoch 29/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0622 - accuracy: 0.1583\n",
      "Epoch 30/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0596 - accuracy: 0.1588\n",
      "Epoch 31/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0557 - accuracy: 0.1599\n",
      "Epoch 32/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0516 - accuracy: 0.1610\n",
      "Epoch 33/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0505 - accuracy: 0.1613\n",
      "Epoch 34/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0474 - accuracy: 0.1621\n",
      "Epoch 35/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0450 - accuracy: 0.1627\n",
      "Epoch 36/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0422 - accuracy: 0.1636\n",
      "Epoch 37/50\n",
      "185/185 [==============================] - 40s 218ms/step - loss: 0.0404 - accuracy: 0.1640\n",
      "Epoch 38/50\n",
      "185/185 [==============================] - 40s 217ms/step - loss: 0.0385 - accuracy: 0.1646\n",
      "Epoch 39/50\n",
      "175/185 [===========================>..] - ETA: 2s - loss: 0.0364 - accuracy: 0.1650"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "history = paper_model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 안녕하세요~~~\n",
      "출력 : 많이 힘들지 않길 바라요 .\n",
      "\n",
      "입력 : 아 퇴근 시간 언제야?\n",
      "출력 : 절대 능력만 중요하지 않아요 .\n",
      "\n",
      "입력 : 왤케 시간이 안 가냐 ㅠ\n",
      "출력 : 말조심하세요 .\n",
      "\n",
      "입력 : 집에 가고 싶다\n",
      "출력 : 즐거운 시간 보내시길 바랍니다 .\n",
      "\n",
      "입력 : 몇 시에 만날래?\n",
      "출력 : 일단 연락처를 주고 받고 부담스럽지 않는 선에서 친해지는게 좋아요 .\n",
      "\n",
      "입력 : 근황 토크 좀 하자\n",
      "출력 : 답답한 상황이네요 .\n",
      "\n",
      "입력 : 거기 개쩌는데잖아\n",
      "출력 : 아이구 .\n",
      "\n",
      "입력 : 나는 헬스를 다니니까\n",
      "출력 : 결정은 그대의 몫입니다 .\n",
      "\n",
      "입력 : 니 이제 인났나?\n",
      "출력 : 후폭풍은 누구에게나 올 거예요 .\n",
      "\n",
      "입력 : 뭐고?\n",
      "출력 : 나랑 같이 놀아요 .\n",
      "\n",
      "입력 : 잤나?\n",
      "출력 : 이해하기 힘드니까요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in test_questions:\n",
    "    sentence_generation(test, paper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 요즘 너무 지쳤어. 어떻게 해야 할까?\n",
      "출력 : 한푼 두푼 차곡차곡차곡\n",
      "\n",
      "입력 : 내가 잘하고 있는지 모르겠어.\n",
      "출력 : 잘하고 있다고 괜찮다고 스스로에게 말해보세요 .\n",
      "\n",
      "입력 : 아무리 노력해도 결과가 안 좋아. 나한테 문제가 있는 걸까?\n",
      "출력 : 안 만나는 게 마음이 편할 거예요 .\n",
      "\n",
      "입력 : 사람들이 날 싫어하는 것 같아.\n",
      "출력 : 감성적이기 딱 좋죠 .\n",
      "\n",
      "입력 : 계속 실패만 해. 포기해야 할까?\n",
      "출력 : 시간이 필요한 것 같아요 .\n",
      "\n",
      "입력 : 누군가에게 기대고 싶은데, 그럴 사람이 없어.\n",
      "출력 : 나이에 상관이 없죠 .\n",
      "\n",
      "입력 : 내 이야기를 들어주는 사람이 없어서 외로워.\n",
      "출력 : 지난 기대에 연연연연해하지 마세요 .\n",
      "\n",
      "입력 : 열심히 하는데도 인정받지 못하는 기분이야.\n",
      "출력 : 살 안 찌는 게 좋을 것 같아요 .\n",
      "\n",
      "입력 : 나는 왜 이렇게 부족할까?\n",
      "출력 : 지금보다 더 잘 살 거예요 .\n",
      "\n",
      "입력 : 다들 행복해 보이는데, 나만 힘든 것 같아.\n",
      "출력 : 새로운 일을 경험해 보세요 .\n",
      "\n",
      "입력 : 내 미래가 불안해서 걱정돼.\n",
      "출력 : 사는 재미가 없을거예요 .\n",
      "\n",
      "입력 : 오늘 너무 힘든 일이 있었어. 위로가 필요해.\n",
      "출력 : 지금 딱 힘들 거예요 .\n",
      "\n",
      "입력 : 무기력하고 아무것도 하기 싫어.\n",
      "출력 : 자신의 몸값을 올려보세요 .\n",
      "\n",
      "입력 : 나는 왜 항상 남들보다 뒤처지는 느낌이지?\n",
      "출력 : 조급하게 생각하지 말아요 .\n",
      "\n",
      "입력 : 요즘 너무 스트레스 받아서 견디기 힘들어.\n",
      "출력 : 저에게 기대세요 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test in wellness_questions:\n",
    "    sentence_generation(test, paper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_text_len(rows):\n",
    "  rows_len = [len(s.split()) for s in rows]\n",
    "  print('최소 길이 : {}'.format(np.min(rows_len)))\n",
    "  print('최대 길이 : {}'.format(np.max(rows_len)))\n",
    "  print('평균 길이 : {}'.format(np.mean(rows_len)))\n",
    "  return rows_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0                         12시 땡 !\n",
       " 1                     1지망 학교 떨어졌어\n",
       " 2                    3박4일 놀러가고 싶다\n",
       " 3                 3박4일 정도 놀러가고 싶다\n",
       " 4                         PPL 심하네\n",
       "                    ...           \n",
       " 11818             훔쳐보는 것도 눈치 보임 .\n",
       " 11819             훔쳐보는 것도 눈치 보임 .\n",
       " 11820                흑기사 해주는 짝남 .\n",
       " 11821    힘든 연애 좋은 연애라는게 무슨 차이일까 ?\n",
       " 11822                  힘들어서 결혼할까봐\n",
       " Name: clean_q, Length: 11823, dtype: object,\n",
       " 0                      하루가 또 가네요 .\n",
       " 1                       위로해 드립니다 .\n",
       " 2                     여행은 언제나 좋죠 .\n",
       " 3                     여행은 언제나 좋죠 .\n",
       " 4                      눈살이 찌푸려지죠 .\n",
       "                    ...            \n",
       " 11818          티가 나니까 눈치가 보이는 거죠 !\n",
       " 11819               훔쳐보는 거 티나나봐요 .\n",
       " 11820                      설렜겠어요 .\n",
       " 11821    잘 헤어질 수 있는 사이 여부인 거 같아요 .\n",
       " 11822          도피성 결혼은 하지 않길 바라요 .\n",
       " Name: clean_a, Length: 11823, dtype: object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = data['clean_q']\n",
    "a = data['clean_a']\n",
    "q, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최소 길이 : 1\n",
      "최대 길이 : 16\n",
      "평균 길이 : 3.9402858834475176\n",
      "최소 길이 : 1\n",
      "최대 길이 : 24\n",
      "평균 길이 : 4.71589275141673\n"
     ]
    }
   ],
   "source": [
    "q_len = get_text_len(q)\n",
    "a_len = get_text_len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbwUlEQVR4nO3de7xXdZ3v8dc7VLQ00SAOcmmjkqVNouGlyTqao6J2Qs+Ul5kUL8VMB9M65hysjlqTE54aLZsycSCoTIZHajLCUYm8jCcvoBIXzeMewQRRMBQ1JxL8zB/ru/PnZv/2Whv2+q0F+/18PNbjt37fdfm9twifvdb6/r5fRQRmZmbdeUvVAczMrP5cLMzMLJeLhZmZ5XKxMDOzXC4WZmaWy8XCzMxyuViYmVkuFwszM8vlYmHWIpLOkrRE0quSnpX0fUm7V53LrAgXC7MWkHQhcAVwEbA7cDjQBtwhaccKo5kVIg/3YVYuSW8HngHOiYhZDe27AsuBL0bEjKrymRXhKwuz8v05sDNwU2NjRLwCzAWOrSKUWU+4WJiVbyDwfERs7GLbamBQi/OY9ZiLhVn5ngcGStqhi21D0nazWnOxMCvffcAG4L83NqZnFscDd1WQyaxHXCzMShYR64GvAt+VNFbSjpLagFlkVxXXV5nPrAj3hjJrEUnnAl8A9gX6A3cDfxURz1QazKwAX1mYtUhETI2I90XEzsA5wD5AV88xzGrHVxZmFZF0BvBaRMysOotZHhcLMzPL5dtQZmaWa7u8Xzpw4MBoa2urOoaZ2TbloYceej4iuvyS6HZZLNra2li4cGHVMczMtimSnmq2zbehzMwsl4uFmZnlcrEwM7NcLhZmZpbLxcLMzHK5WJiZWS4XCzMzy+ViYWZmuVwszMws13b5De7tVdukOU23rZh8YguTmFlfU9qVhaSdJT0o6deSlkn6amofKekBSe2S/kXSTqm9f3rfnra3NZzr4tT+uKTjyspsZmZdK/M21AbgoxFxIDAaGCvpcOAK4KqI2Bd4ATg37X8u8EJqvyrth6T9gdOAA4CxwPcl9Ssxt5mZdVJasYjMK+ntjmkJ4KPAz1L7DOCktD4uvSdtP1qSUvvMiNgQEcuBduDQsnKbmdnmSn3ALamfpEXAGmAe8O/AixGxMe2yEhia1ocCTwOk7euBdzS2d3FM42dNkLRQ0sK1a9eW8NOYmfVdpRaLiNgUEaOBYWRXA+8p8bOmRMSYiBgzaFCXw7GbmdkWaknX2Yh4EbgT+CAwQFJHL6xhwKq0vgoYDpC27w78rrG9i2PMzKwFyuwNNUjSgLS+C3AM8BhZ0fhE2m08cEtan53ek7b/MrIJwmcDp6XeUiOBUcCDZeU2M7PNlfk9iyHAjNRz6S3ArIi4VdKjwExJXwceAaam/acCP5bUDqwj6wFFRCyTNAt4FNgITIyITSXmNjOzTkorFhGxGDioi/Yn6aI3U0T8Afhkk3NdDlze2xnNzKwYD/dhZma5XCzMzCyXi4WZmeVysTAzs1wuFmZmlsvFwszMcrlYmJlZLhcLMzPL5WJhZma5XCzMzCyXi4WZmeVysTAzs1wuFmZmlsvFwszMcrlYmJlZLhcLMzPL5WJhZma5XCzMzCyXi4WZmeVysTAzs1wuFmZmlsvFwszMcrlYmJlZLhcLMzPL5WJhZma5SisWkoZLulPSo5KWSbogtV8maZWkRWk5oeGYiyW1S3pc0nEN7WNTW7ukSWVlNjOzru1Q4rk3AhdGxMOSdgMekjQvbbsqIr7VuLOk/YHTgAOAvYBfSHp32vw94BhgJbBA0uyIeLTE7GZm1qC0YhERq4HVaf1lSY8BQ7s5ZBwwMyI2AMsltQOHpm3tEfEkgKSZaV8Xi17UNmlO020rJp/YwiRmVkcteWYhqQ04CHggNZ0nabGkaZL2SG1DgacbDluZ2pq1d/6MCZIWSlq4du3a3v4RzMz6tNKLhaRdgRuBz0fES8A1wD7AaLIrj3/sjc+JiCkRMSYixgwaNKg3TmlmZkmZzyyQtCNZobg+Im4CiIjnGrZfB9ya3q4ChjccPiy10U27mZm1QJm9oQRMBR6LiCsb2oc07HYysDStzwZOk9Rf0khgFPAgsAAYJWmkpJ3IHoLPLiu3mZltrswriw8BZwBLJC1KbV8CTpc0GghgBfA3ABGxTNIssgfXG4GJEbEJQNJ5wO1AP2BaRCwrMbeZmXVSZm+oewF1sWluN8dcDlzeRfvc7o4zM7Ny+RvcZmaWy8XCzMxyuViYmVkuFwszM8vlYmFmZrlcLMzMLJeLhZmZ5XKxMDOzXLnFQtIn03wUSPqKpJskHVx+NDMzq4siVxb/O81HcQTwF2TjPV1TbiwzM6uTIsViU3o9EZgSEXOAncqLZGZmdVOkWKySdC1wKjBXUv+Cx5mZ2XaiyD/6p5CN+HpcRLwI7AlcVGYoMzOrl9xiERGvAmuAI1LTRuCJMkOZmVm9FOkNdSnwv4CLU9OOwE/KDGVmZvVS5DbUycDHgd8DRMQzwG5lhjIzs3opUiz+GBFBNrMdkt5WbiQzM6ubIsViVuoNNUDSZ4BfANeVG8vMzOokd1rViPiWpGOAl4D9gEsiYl7pyczMrDYKzcGdioMLhJlZH9W0WEh6mfScovMmICLi7aWlMjOzWmlaLCLCPZ7MzAwoeBsqjTJ7BNmVxr0R8UipqczMrFaKfCnvEmAG8A5gIDBd0lfKDmZmZvVR5Mrir4EDI+IPAJImA4uAr3d3kKThwI+AwWRXJFMi4juS9gT+BWgDVgCnRMQLkgR8BzgBeBU4KyIeTucaD3QUqK9HxIwe/Iwt1TZpTtNtKyaf2MIkZma9p8j3LJ4Bdm543x9YVeC4jcCFEbE/cDgwUdL+wCRgfkSMAuan9wDHA6PSMoE0Z0YqLpcChwGHApdK2qPA55uZWS8pUizWA8skTZf0Q2Ap8KKkqyVd3eygiFjdcWUQES8DjwFDgXFkt7VIryel9XHAjyJzP9mXAIcAxwHzImJdRLxA1oV3bE9/UDMz23JFbkPdnJYOd/X0QyS1AQcBDwCDI2J12vQs2W0qyArJ0w2HrUxtzdrNzKxFinyDe6ueD0jaFbgR+HxEvJQ9mvjTuUNSV9/l2JLPmUB2+4oRI0b0xinNzCwp0hvqY5IekbRO0kuSXpb0UpGTS9qRrFBcHxE3pebn0u0l0uua1L4KGN5w+LDU1qz9TSJiSkSMiYgxgwYNKhLPzMwKKvLM4tvAeOAdEfH2iNityLe3U++mqcBjEXFlw6bZ6Xyk11sa2s9U5nBgfbpddTtwrKQ90oPtY1ObmZm1SJFnFk8DS9Mw5T3xIeAMYImkRantS8BkspFszwWeIpu2FWAuWbfZdrKus2cDRMQ6SX8PLEj7fS0i1vUwi5mZbYUixeLvgLmS7gY2dDR2ulrYTETcSzaOVFeO7mL/ACY2Odc0YFqBrGZmVoIixeJy4BWy71rsVG4cMzOroyLFYq+IeF/pSczMrLaKPOCeK+nY0pOYmVltFSkWnwVuk/QfPe06a2Zm24ciX8rzvBZmZn1c0fks9iAb4O9PAwpGxD1lhTIzs3rJLRaSPg1cQPbN6UVkI8jeB3y01GRmZlYbRZ5ZXAAcAjwVEUeRDQj4YpmhzMysXooUiz80THzUPyJ+A+xXbiwzM6uTIs8sVkoaAPwcmCfpBbJhOszMrI8o0hvq5LR6maQ7gd2B20pNZWZmtVJkiPJ9JPXveEs2d/ZbywxlZmb1UuSZxY3AJkn7AlPI5pb4aampzMysVooUi9cjYiNwMvDdiLgIGFJuLDMzq5MixeI1SaeTTVR0a2rbsbxIZmZWN0WKxdnAB4HLI2K5pJHAj8uNZWZmdVKkN9SjwPkN75cDV5QZyszM6qXIlYWZmfVxLhZmZparabGQ9OP0ekHr4piZWR11d2XxAUl7AedI2kPSno1LqwKamVn1unvA/QNgPrA38BDZt7c7RGo3o23SnKbbVkw+sYVJzKwsTa8sIuLqiHgvMC0i9o6IkQ2LC4WZWR9SpOvsZyUdCHw4Nd0TEYvLjWVmZnVSZCDB84HrgXem5XpJnys7mJmZ1UeRrrOfBg6LiEsi4hKyaVU/k3eQpGmS1kha2tB2maRVkhal5YSGbRdLapf0uKTjGtrHprZ2SZN69uOZmVlvKFIsBGxqeL+JNz/sbmY6MLaL9qsiYnRa5gJI2h84DTggHfN9Sf0k9QO+BxwP7A+cnvY1M7MWKjJT3g+BByTdnN6fBEzNOygi7pHUVjDHOGBmRGwAlktqBw5N29oj4kkASTPTvo8WPK+ZmfWC3CuLiLiSbDDBdWk5OyK+vRWfeZ6kxek21R6pbSjwdMM+K1Nbs/bNSJogaaGkhWvXrt2KeGZm1lmh4T4i4uHUlfbqiHhkKz7vGmAfYDSwGvjHrTjXm0TElIgYExFjBg0a1FunNTMzit2G6jUR8VzHuqTreGN+jFVkM/B1GJba6KbdzMxapKUDCUpqnGHvZKCjp9Rs4DRJ/dN8GaOAB4EFwChJIyXtRPYQfHYrM5uZWc6VReqN9IuIOKqnJ5Z0A3AkMFDSSuBS4EhJo8mGC1kB/A1ARCyTNIvswfVGYGJEbErnOQ+4HehH9m3yZT3NYmZmW6fbYhERmyS9Lmn3iFjfkxNHxOldNDftRRURlwOXd9E+F5jbk882M7PeVeSZxSvAEknzgN93NEbE+c0PMTOz7UmRYnFTWszMrI8qMpDgDEm7ACMi4vEWZDIzs5opMpDgfwMWAbel96MluUeSmVkfUqTr7GVkQ2+8CBARi/DER2ZmfUqRYvFaFz2hXi8jjJmZ1VORB9zLJP0V0E/SKOB84FflxjIzszopcmXxObKhwzcANwAvAZ8vMZOZmdVMkd5QrwJflnRF9jZeLj+WmZnVSZHeUIdIWgIsJvty3q8lfaD8aGZmVhdFnllMBf5HRPwbgKQjyCZEen+ZwczMrD6KPLPY1FEoACLiXrLB/szMrI9oemUh6eC0ereka8kebgdwKnBX+dHMzKwuursN1XkWu0sb1qOELGZmVlNNi8WWzGFhZmbbp9wH3JIGAGcCbY37e4hyM7O+o0hvqLnA/cASPMyHmVmfVKRY7BwR/7P0JGZmVltFus7+WNJnJA2RtGfHUnoyMzOrjSJXFn8Evgl8mTd6QQUeptzMrM8oUiwuBPaNiOfLDmNmZvVU5DZUO/Bq2UHMzKy+ilxZ/B5YJOlOsmHKAXedNTPrS4oUi5+nxczM+qgi81nMaEUQMzOrryLzWSyX9GTnpcBx0yStkbS0oW1PSfMkPZFe90jtknS1pHZJixsGMUTS+LT/E5LGb+kPamZmW67IA+4xwCFp+TBwNfCTAsdNB8Z2apsEzI+IUcD89B7geGBUWiYA10BWXMgGMDwMOBS4tKPAmJlZ6+QWi4j4XcOyKiK+DZxY4Lh7gHWdmscBHbe1ZgAnNbT/KDL3AwMkDQGOA+ZFxLqIeAGYx+YFyMzMSlZkIMGDG96+hexKo8iD8a4MjojVaf1ZYHBaHwo83bDfytTWrL2rnBPIrkoYMWLEFsYzM7OuFPlHv3Fei43ACuCUrf3giAhJvTYvRkRMAaYAjBkzxvNtmJn1oiK9oXpzXovnJA2JiNXpNtOa1L4KGN6w37DUtgo4slP7Xb2Yx8zMCihyG6o/8JdsPp/F17bg82YD44HJ6fWWhvbzJM0ke5i9PhWU24F/aHiofSxw8RZ8rpmZbYUit6FuAdYDD9HwDe48km4guyoYKGklWa+mycAsSecCT/HG7ay5wAm8MbTI2QARsU7S3wML0n5fi4jOD81tO9Y2aU7TbSsm5/azMLNeUqRYDIuIHvdAiojTm2w6uot9A5jY5DzTgGk9/XwzM+s9Rb5n8StJf1Z6EjMzq60iVxZHAGdJWk52G0pkFwPvLzWZmZnVRpFicXzpKczMrNaKdJ19qhVBzMysvoo8szAzsz7OxcLMzHK5WJiZWS4XCzMzy+ViYWZmuVwszMwsl4uFmZnlcrEwM7NcLhZmZpbLxcLMzHK5WJiZWS4XCzMzy+ViYWZmuVwszMwsl4uFmZnlcrEwM7NcLhZmZpbLxcLMzHK5WJiZWa7cObjNtkdtk+Z0u33F5BNblMRs2+ArCzMzy1VJsZC0QtISSYskLUxte0qaJ+mJ9LpHapekqyW1S1os6eAqMpuZ9WVVXlkcFRGjI2JMej8JmB8Ro4D56T3A8cCotEwArml5UjOzPq5Ot6HGATPS+gzgpIb2H0XmfmCApCEV5DMz67OqesAdwB2SArg2IqYAgyNiddr+LDA4rQ8Fnm44dmVqW93QhqQJZFcejBgxYqvCdffw0w8+zawvqqpYHBERqyS9E5gn6TeNGyMiUiEpLBWcKQBjxozp0bFmZta9Sm5DRcSq9LoGuBk4FHiu4/ZSel2Tdl8FDG84fFhqMzOzFml5sZD0Nkm7dawDxwJLgdnA+LTbeOCWtD4bODP1ijocWN9wu8rMzFqgittQg4GbJXV8/k8j4jZJC4BZks4FngJOSfvPBU4A2oFXgbNbH9nMrG9rebGIiCeBA7to/x1wdBftAUxsQTQzM2uiTl1nzcysplwszMwsl4uFmZnlcrEwM7NcLhZmZpbLxcLMzHK5WJiZWS7PlGe2BTzYpPU1vrIwM7NcLhZmZpbLxcLMzHK5WJiZWS4XCzMzy+ViYWZmuVwszMwsl4uFmZnlcrEwM7NcLhZmZpbLw32Y1Uh3w4iAhxKx6vjKwszMcrlYmJlZLhcLMzPL5WJhZma5/IDbrI/ww3PbGr6yMDOzXNtMsZA0VtLjktolTao6j5lZX7JN3IaS1A/4HnAMsBJYIGl2RDxabTKzvsNTyfZt20SxAA4F2iPiSQBJM4FxgIuF2TauzGcpfk7TexQRVWfIJekTwNiI+HR6fwZwWESc17DPBGBCersf8HjLgzY3EHi+6hA56p6x7vmg/hnrng/qn7Hu+WDrMr4rIgZ1tWFbubLIFRFTgClV5+iKpIURMabqHN2pe8a654P6Z6x7Pqh/xrrng/IybisPuFcBwxveD0ttZmbWAttKsVgAjJI0UtJOwGnA7IozmZn1GdvEbaiI2CjpPOB2oB8wLSKWVRyrJ2p5e6yTumesez6of8a654P6Z6x7Pigp4zbxgNvMzKq1rdyGMjOzCrlYmJlZLheLEkkaLulOSY9KWibpgqozdUVSP0mPSLq16ixdkTRA0s8k/UbSY5I+WHWmRpK+kP58l0q6QdLONcg0TdIaSUsb2vaUNE/SE+l1jxpm/Gb6c14s6WZJA+qUr2HbhZJC0sAqsjXk6DKjpM+l/47LJP2f3vgsF4tybQQujIj9gcOBiZL2rzhTVy4AHqs6RDe+A9wWEe8BDqRGWSUNBc4HxkTE+8g6YJxWbSoApgNjO7VNAuZHxChgfnpfpelsnnEe8L6IeD/w/4GLWx2qwXQ2z4ek4cCxwG9bHagL0+mUUdJRZCNcHBgRBwDf6o0PcrEoUUSsjoiH0/rLZP/IDa021ZtJGgacCPxz1Vm6Iml34CPAVICI+GNEvFhpqM3tAOwiaQfgrcAzFechIu4B1nVqHgfMSOszgJNamamzrjJGxB0RsTG9vZ/sO1WVaPLfEOAq4O+AynsHNcn4WWByRGxI+6zpjc9ysWgRSW3AQcADFUfp7Ntk/+O/XnGOZkYCa4Efpltl/yzpbVWH6hARq8h+c/stsBpYHxF3VJuqqcERsTqtPwsMrjJMAecA/7fqEI0kjQNWRcSvq87SjXcDH5b0gKS7JR3SGyd1sWgBSbsCNwKfj4iXqs7TQdLHgDUR8VDVWbqxA3AwcE1EHAT8nupvn/xJuu8/jqyo7QW8TdKnqk2VL7I+85X/ZtyMpC+T3ca9vuosHSS9FfgScEnVWXLsAOxJduv7ImCWJG3tSV0sSiZpR7JCcX1E3FR1nk4+BHxc0gpgJvBRST+pNtJmVgIrI6LjiuxnZMWjLv4CWB4RayPiNeAm4M8rztTMc5KGAKTXXrk90dsknQV8DPjrqNcXwfYh+6Xg1+nvzDDgYUn/pdJUm1sJ3BSZB8nuGmz1g3gXixKlaj4VeCwirqw6T2cRcXFEDIuINrKHsr+MiFr9VhwRzwJPS9ovNR1NvYam/y1wuKS3pj/vo6nRA/hOZgPj0/p44JYKs3RJ0liy26Ifj4hXq87TKCKWRMQ7I6It/Z1ZCRyc/h+tk58DRwFIejewE70wUq6LRbk+BJxB9hv7orScUHWobdDngOslLQZGA/9QbZw3pCuenwEPA0vI/k5VPiSEpBuA+4D9JK2UdC4wGThG0hNkV0STa5jxn4DdgHnp78sPapavVppknAbsnbrTzgTG98YVmof7MDOzXL6yMDOzXC4WZmaWy8XCzMxyuViYmVkuFwszM8vlYmHbPEmvlHDO0Y3dnCVdJumLW3G+T6YRc+/snYRbnGNF1SOl2rbJxcKsa6OB3vxOzLnAZyLiqF48p1nLuFjYdkXSRZIWpPkQvpra2tJv9del8f3vkLRL2nZI2ndRmkthqaSdgK8Bp6b2U9Pp95d0l6QnJZ3f5PNPl7QkneeK1HYJcAQwVdI3O+0/RNI96XOWSvpwar9G0sKU96sN+6+Q9I20/0JJB0u6XdK/S/rbtM+R6ZxzJD0u6QeSNvu7LulTkh5M57pW2bwm/SRNT1mWSPrCVv6R2PYiIrx42aYX4JX0eizZt6dF9ovQrWTDm7eRDUo3Ou03C/hUWl8KfDCtTwaWpvWzgH9q+IzLgF8B/cnG2fkdsGOnHHuRDf8xiGwwt18CJ6Vtd5HNedE5+4XAl9N6P2C3tL5nQ9tdwPvT+xXAZ9P6VcBism88DwKeS+1HAn8A9k7HzwM+0XD8QOC9wL92/AzA94EzgQ8A8xryDaj6z9dLPRZfWdj25Ni0PEI2/MZ7gFFp2/KIWJTWHwLalM3CtltE3Jfaf5pz/jkRsSEinicbhK/zEN+HAHdFNqhgx4ipH8k55wLgbEmXAX8W2bwnAKdIejj9LAcAjZNmzU6vS4AHIuLliFgLbNAbM8s9GBFPRsQm4AayK5tGR5MVhgWSFqX3ewNPkg0V8d00TlNtRkm2au1QdQCzXiTgGxFx7Zsas7lENjQ0bQJ22YLzdz7HVv/9iYh7JH2EbAKq6ZKuBP4N+CJwSES8IGk60DhVa0eO1ztler0hU+dxfDq/FzAjIjabiU7SgcBxwN8Cp5DNK2F9nK8sbHtyO3BOmj8ESUMlvbPZzpHNuPeypMNSU+N0qC+T3d7piQeB/yppoKR+wOnA3d0dIOldZLePriObrfBg4O1k83aslzQYOL6HOQAOlTQyPas4Fbi30/b5wCc6/vsom5/7Xamn1Fsi4kbgK9RrOHirkK8sbLsREXdIei9wXzZaOK8AnyK7CmjmXOA6Sa+T/cO+PrXfCUxKt2i+UfDzV0ualI4V2W2rvGHAjwQukvRayntmRCyX9AjwG+Bp4P8V+fxOFpCN4LpvynNzp6yPSvoKcEcqKK8BE4H/IJuVsOMXySrnwLYa8aiz1qdJ2jUiXknrk4AhEXFBxbG2iqQjgS9GxMcqjmLbEV9ZWF93oqSLyf4uPEXWC8rMOvGVhZmZ5fIDbjMzy+ViYWZmuVwszMwsl4uFmZnlcrEwM7Nc/wnQ+++5maGBmAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(q_len)\n",
    "plt.title('Q')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(a_len)\n",
    "plt.title('A')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Q')\n",
    "plt.hist(q_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('A')\n",
    "plt.hist(a_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고\n",
    "#### 실험 하이퍼파라미터\n",
    "- Layer 개수와 차원 수\n",
    "  1. `NUM_LAYERS = 2`, `D_MODEL = 256`\n",
    "  2. `NUM_LAYERS = 6`, `D_MODEL = 512`\n",
    "- EPOCHS\n",
    "  1. 10\n",
    "  2. 50\n",
    "- 시간 관계상 논문에 나온 하이퍼파라미터가 아닌 적은 수를 썼다고 해서 비교해 봄\n",
    "  - 근데 의외로 성능이 제일 좋은 건 1 + 2의 조합 (`NUM_LAYERS = 2`, `D_MODEL = 256`, `EPOCHS = 50`)\n",
    "- 데이터의 최대 길이랑 평균 길이를 출력해 봤는데 평균 길이 자체가 굉장히 짧음\n",
    "    ```\n",
    "    최소 길이 : 1\n",
    "    최대 길이 : 16\n",
    "    평균 길이 : 3.9402858834475176\n",
    "    최소 길이 : 1\n",
    "    최대 길이 : 24\n",
    "    평균 길이 : 4.71589275141673\n",
    "    ```\n",
    "  - 데이터가 단순한데 페이퍼 모델을 너무 복잡해서 그럴 수도..\n",
    "- 또, 한국어가 형태소 위주로 끊어야 되는데, 그게 아니라 많은 정보가 손실 됐을 수도..\n",
    "- `MAX_LENGTH = 40`으로 했는데 이유를 생각해보니,\n",
    "  - 일단 트랜스포머는 병렬로 들어가니깐 문장들을 최대한 살리기 위함\n",
    "  - 디코더가 생성하는 문장도 역시 해당 파라미터를 고려하니 어색하지 않게 문장을 생성하기 위함\n",
    "- 따라서 해 볼 추가 실험\n",
    "  1. `MAX_LENGTH` 조절해보기\n",
    "  2. 형태소로 자르는거 해보기\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
